{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/b3GOR/Flang/blob/main/FLang.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lhHO54Ogw_Bz"
      },
      "source": [
        "# Загрузка библиотек и модели\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sIC1YCs9U4Jp"
      },
      "outputs": [],
      "source": [
        "%%capture\n",
        "!pip install transformers accelerate"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Загрузка необходимых библиотек\n",
        "from transformers import AutoTokenizer\n",
        "import transformers\n",
        "import torch\n",
        "from google.colab import files"
      ],
      "metadata": {
        "id": "_uPTu8L2-dYC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BgD-ZQZ_U_qQ"
      },
      "outputs": [],
      "source": [
        "# Загрузка модели\n",
        "model = \"b3G0R/FLang\"\n",
        "tokenizer = AutoTokenizer.from_pretrained(model)\n",
        "pipeline = transformers.pipeline(\n",
        "    \"text-generation\",\n",
        "    model=model,\n",
        "    torch_dtype=torch.float16,\n",
        "    device_map=\"auto\",\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Загрузка пользовательского словаря"
      ],
      "metadata": {
        "id": "Igj69eMzK4Hr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Загрузка словаря пользователя из слов, которые он уже знает\n",
        "uploaded = files.upload()\n",
        "file_names = list(uploaded.keys())\n",
        "first_file_name = file_names[0]\n",
        "with open(first_file_name, 'r') as file:\n",
        "  w = file.read().split()\n",
        "  dct = ', '.join(w)"
      ],
      "metadata": {
        "id": "6p498a0x_kvc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Генератор промтов, функция скорриноговой модели"
      ],
      "metadata": {
        "id": "UFhYDkqn7tgc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Массив промптов для генерации нового промпта в резульатате комбинации\n",
        "v = ['Write a text on English of {sentence} sentence about {topic} topic, but use these words:{words}.',\n",
        "\n",
        "'Create a passage in English with {sentence} sentence(s) discussing the topic of {topic}. Make sure to include the words:{words}.',\n",
        "\n",
        "'Compose an English text comprising {sentence} sentence(s) that revolves around the subject of {topic}. Incorporate the words:{words} into your writing.',\n",
        "\n",
        "'Craft a piece of writing in English that consists of {sentence} sentence(s) and centers on the topic of {topic}. Ensure you incorporate the words:{words} into your composition.',\n",
        "\n",
        "'Write a short English essay with {sentence} sentence(s) addressing the {topic} topic. You must utilize the words:{words} in your writing.',\n",
        "\n",
        "'Develop an English narrative containing {sentence} sentence(s) exploring the subject of {topic}. Be sure to include the words:{words} in your narrative.',\n",
        "\n",
        "'Compose a paragraph in English consisting of {sentence} sentence(s) that discusses the {topic} topic. Make sure to use the words:{words} in your paragraph.',\n",
        "\n",
        "'Create an English composition with {sentence} sentence(s) that delves into the subject of {topic}. Incorporate the words:{words} into your composition.',\n",
        "\n",
        "'Write an English text with {sentence} sentence(s) focusing on the {topic} topic. Your text should feature the words:{words}.',\n",
        "\n",
        "'Craft an English essay comprising {sentence} sentence(s) that explores the {topic} topic. Make sure to include the words:{words} in your essay.']"
      ],
      "metadata": {
        "id": "JPCALiEBcUcZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Генерирование stopwords:\n",
        "\n",
        "# stopwords spacy\n",
        "import spacy\n",
        "#loading the english language small model of spacy\n",
        "en = spacy.load('en_core_web_sm')\n",
        "stopwords_spacy = set(en.Defaults.stop_words)\n",
        "\n",
        "# stopwords nltk\n",
        "import nltk\n",
        "nltk.download('stopwords')\n",
        "from nltk.corpus import stopwords\n",
        "stopwords_nltk = set(stopwords.words('english'))\n",
        "\n",
        "# stopwords sklearn\n",
        "from sklearn.feature_extraction.text import ENGLISH_STOP_WORDS\n",
        "stopwords_sklearn = set(ENGLISH_STOP_WORDS)\n",
        "\n",
        "# stopwords gensim\n",
        "from gensim.parsing.preprocessing import STOPWORDS\n",
        "stopwords_gensim = set(STOPWORDS)\n",
        "\n",
        "# объединим стоп-слова\n",
        "stopwords_all = stopwords_gensim.union(stopwords_sklearn, stopwords_nltk, stopwords_spacy)\n",
        "\n",
        "# Создадим множество из лексем стоп-слов через spacy (на всякий случай)\n",
        "nlp = spacy.load('en_core_web_sm')\n",
        "stopwords_all_lex = set() # Создадим множество из лексем стоп-слов через spacy (на всякий случай)\n",
        "for _ in stopwords_all:\n",
        "  stopwords_all_lex.add(str(nlp(_)).lower())\n",
        "\n",
        "# Добавим знаки препинания в stopwords_all_lex\n",
        "stopwords_all_lex.update((\".\", \",\", \";\", \"'\", \"...\", \"(\", \")\", \"[\", \"]\", \":\", \"-\", \"?\", \"!\", \"\\\"\"))"
      ],
      "metadata": {
        "id": "pvwL_tnPVJBi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Создадим множество dct_set и добавим туда токены из dct (кроме запятых)\n",
        "dct_set = set()\n",
        "for _ in dct.split(\",\"):\n",
        "  dct_set.add(str(nlp(_.strip())).lower())"
      ],
      "metadata": {
        "id": "j8EN0wgvZDTR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Генерация промпта на основе словаря промптов путем их случайной комбинации\n",
        "import numpy as np\n",
        "\n",
        "d = np.array(v)\n",
        "def generate_prompt():\n",
        "    start = ['Create an English','Compose an English text','Craft an English','Write a text on English','Develop an English']\n",
        "    middle = ['{sentence}', '{topic}', 'but use these words:{words}']\n",
        "    words = np.random.permutation(len(d))[:1]\n",
        "    target_word = d[np.random.randint(0, len(d))]\n",
        "    return f\"{start[np.random.randint(0, len(start))]} - {words} - {middle[np.random.randint(0, len(middle))]} - {target_word}\"\n",
        "\n",
        "d =generate_prompt().split('-')[-1].strip()\n",
        "print(d)"
      ],
      "metadata": {
        "id": "t1qgIs0Nbu4c"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Генератор текста к словам"
      ],
      "metadata": {
        "id": "Exd_Upu88H58"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "s6_NL_W5xsTE",
        "cellView": "form"
      },
      "outputs": [],
      "source": [
        "# @title Генератор текста к словам\n",
        "topic = \"\" # @param {type:\"string\"}\n",
        "sentences = \"\" # @param {type:\"string\"}\n",
        "level = \"A1\" # @param [\"A1\", \"A2\", \"B1\", \"B2\", \"C1\", \"C2\"]\n",
        "words = \"\" # @param {type:\"string\"}\n",
        "\n",
        "r = d.replace('{sentence}', sentences).replace('{topic}', topic).replace('{words}', words)\n",
        "prompt = r\n",
        "# Системный промпт и обычный промпт\n",
        "SYS = f'''You are an English teacher, and your task is to compose a well-written English text that demonstrates strong grammar, style, and semantics.\n",
        "The text should be tailored to an English proficiency level of {level}.\n",
        "The text must consist of only these words ({dct}) and these words {words}.'''\n",
        "# Добавление в словарь уже изученных слов, новые слова, которые ввел пользователь\n",
        "dct_n = dct + ', ' + words\n",
        "del  dct\n",
        "dct = dct_n\n",
        "del dct_n\n",
        "\n",
        "\n",
        "####### Создадим множество words_set и добавим туда токены из words. Эти же токены\n",
        "####### добавим в dct_set\n",
        "\n",
        "word_set = set()\n",
        "for _ in words.split(\",\"):\n",
        "  word_set.add(str(nlp(_.strip())).lower())\n",
        "  dct_set.add(str(nlp(_.strip())).lower())\n",
        "#######\n",
        "# Генерация текста\n",
        "sequences = pipeline(\n",
        "    f'[INST] <<SYS>> {SYS}  <</SYS>> {prompt} [/INST]',\n",
        "    do_sample=True,\n",
        "    top_k=10,\n",
        "    num_return_sequences=1,\n",
        "    eos_token_id=tokenizer.eos_token_id,\n",
        "    max_length=500,\n",
        ")\n",
        "\n",
        "generated_text = sequences[0]['generated_text']\n",
        "\n",
        "# Удаление информации о введенном промпте (на выходе только текст)\n",
        "while \"[INST]\" in generated_text:\n",
        "    start_index = generated_text.find(\"[INST]\")\n",
        "    end_index = generated_text.find(\"[/INST]\", start_index)\n",
        "\n",
        "    if start_index != -1 and end_index != -1:\n",
        "        generated_text = generated_text[:start_index] + generated_text[end_index + 7:]\n",
        "    else:\n",
        "        break\n",
        "\n",
        "# Разделяем предложения символом новой строки\n",
        "generated_text = generated_text.replace(\".\", \".\\n\")\n",
        "\n",
        "print(generated_text)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Скоринговая модель"
      ],
      "metadata": {
        "id": "oMxRXHUM7YMb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "######\n",
        "###### Функция reward_scoring показывает долю токенов\n",
        "###### из dct_set(words_study) и stopwords_all_lex(stopwords_set) в сгенерированной\n",
        "###### фразе. В случае, если во фразе отсутствуют слова из word_set, возвращаемое значение\n",
        "###### функции reward_scoring домножается на 0.1 ** (число отсутствующих слов из word_set во фразе)\n",
        "\n",
        "######\n",
        "def reward_scoring(sentence: str, target_word: set,\n",
        "                   words_study: set, stopwords_set:set) -> float:\n",
        "  sentence_lex = []\n",
        "  doc = nlp(sentence)\n",
        "  for token in doc:\n",
        "    sentence_lex.append(str(token.lemma_).lower())\n",
        "\n",
        "  count_words_in_sets = 0 # число слов в множествах words_study и stopwords\n",
        "  for _ in sentence_lex:\n",
        "    if _ in words_study or _ in stopwords_set:\n",
        "      count_words_in_sets += 1\n",
        "  rew_score = count_words_in_sets/len(sentence_lex)\n",
        "\n",
        "  count_target_word = 0 # проверяем, сколько таргет-слов есть в sentence\n",
        "  for _ in target_word:\n",
        "    if _ in sentence_lex:\n",
        "      count_target_word += 1\n",
        "\n",
        "  rew_score *= (0.1 ** (len(target_word) - count_target_word))\n",
        "  return rew_score"
      ],
      "metadata": {
        "id": "BBRPFD_3aoIq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "###### Расчет rew_score\n",
        "\n",
        "sentence = \"The dog is sitting near my daughter.\" ### Введите в кавычках сгенерированную фразу\n",
        "reward_scoring(sentence, word_set, dct_set, stopwords_all_lex)"
      ],
      "metadata": {
        "id": "ci9z_-BGfoX-",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "230ca167-5ce5-4178-d9c8-d1c39b7f566f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.08750000000000001"
            ]
          },
          "metadata": {},
          "execution_count": 36
        }
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "collapsed_sections": [
        "lhHO54Ogw_Bz",
        "Igj69eMzK4Hr",
        "UFhYDkqn7tgc",
        "Exd_Upu88H58",
        "oMxRXHUM7YMb"
      ],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}